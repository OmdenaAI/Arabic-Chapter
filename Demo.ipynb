{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1f1e76-aab8-420a-a32b-5e7793b75aef",
   "metadata": {},
   "source": [
    "<h1>Task 8: Word Embedding</h1>\n",
    "\n",
    "<h4> This notebook compares different embedding methods on a simple task (sentiment analysis) <a href=\"https://www.kaggle.com/mksaad/arabic-sentiment-twitter-corpus\">on a small dataset</a>.</h4>\n",
    "\n",
    "<h4>Table of Contents:</h4>\n",
    "<ol>\n",
    "    <li>Load Dataset</li>\n",
    "    <li>Normalize Dataset</li>\n",
    "    <li>Tokenize Dataset</li>\n",
    "    <li>Word Embedding</li>\n",
    "    <li>Train RNN model</li>\n",
    "    <li>Evaluate model</li>\n",
    "</ol>\n",
    "<h4>Embedding Methods:</h4>\n",
    "<ol>\n",
    "    <li>Keras Embedding Layer (trained from scratch)</li>\n",
    "    <li>Keras Word2Vec implementation (trained from scratch)</li>\n",
    "    <li>Genism library's Word2Vec implementation (trained from scratch)</li>\n",
    "    <li>Genism library's GloVe implementation (trained from scratch)</li>\n",
    "    <li>Genism library's fasttext implementation (trained from scratch)</li>\n",
    "    <li>AraVec pretrained embeddings</li>\n",
    "    <li>Arabic-Chapter pretrained embeddings</li>\n",
    "    <li>BERT Arabic pretrained model</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cd5e5-08d7-4614-97cf-90e86c805069",
   "metadata": {},
   "source": [
    "<h1>Load Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0753d856-c632-4660-b903-512d7385b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_pos = pd.read_csv(\"data/train_Arabic_tweets_positive_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "train_neg = pd.read_csv(\"data/train_Arabic_tweets_negative_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "test_pos = pd.read_csv(\"data/test_Arabic_tweets_positive_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "test_neg = pd.read_csv(\"data/test_Arabic_tweets_negative_20190413.tsv\", sep='\\t', names=[\"label\", \"tweet\"])\n",
    "train = pd.concat([train_pos, train_neg])#.sample(frac=1, random_state=0)\n",
    "test = pd.concat([test_pos, test_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf0ac73-68f4-4f33-ad8a-49e161e566dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰Ù° Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰Ù° Ø§Ù„Ø¬Ù…Ø§Ù„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ ğŸ’›</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>#Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ : Ø§Ù„Ù…Ø³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>neg</td>\n",
       "      <td>ÙƒÙŠÙ ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ Ù„Ùˆ ÙƒØ§Ù† ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø± ØŸ ğŸ’™ğŸ’™ ÙƒÙˆÙƒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>neg</td>\n",
       "      <td>Ø§Ø­Ø³Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙ… ğŸ’”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>neg</td>\n",
       "      <td>Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ø§ Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ ğŸ’”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>neg</td>\n",
       "      <td>Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙŠØ§ ÙˆØ§Ø·ÙŠ ğŸ¤”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>neg</td>\n",
       "      <td>Ù‚Ø¯ Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ù†ÙˆÙ‰ Ø¥Ø° ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§ Ø› ØºØ±ÙŠØ¨Ø§ Ø¨...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet\n",
       "0       pos  Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...\n",
       "1       pos  ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰Ù° Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰Ù° Ø§Ù„Ø¬Ù…Ø§Ù„...\n",
       "2       pos                                    Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ ğŸ’›\n",
       "3       pos  #Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶...\n",
       "4       pos  Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ : Ø§Ù„Ù…Ø³...\n",
       "...     ...                                                ...\n",
       "22509   neg  ÙƒÙŠÙ ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ Ù„Ùˆ ÙƒØ§Ù† ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø± ØŸ ğŸ’™ğŸ’™ ÙƒÙˆÙƒ...\n",
       "22510   neg                                  Ø§Ø­Ø³Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙ… ğŸ’”\n",
       "22511   neg                            Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ø§ Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ ğŸ’”\n",
       "22512   neg                                 Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙŠØ§ ÙˆØ§Ø·ÙŠ ğŸ¤”\n",
       "22513   neg  Ù‚Ø¯ Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ù†ÙˆÙ‰ Ø¥Ø° ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§ Ø› ØºØ±ÙŠØ¨Ø§ Ø¨...\n",
       "\n",
       "[45275 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec34d35c-034b-4d11-8ea6-6767653d47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize(text):\n",
    "    text = araby.strip_harakat(text)\n",
    "    text = araby.strip_tashkeel(text)\n",
    "    text = araby.strip_small(text)\n",
    "    text = araby.strip_tatweel(text)\n",
    "    text = araby.strip_shadda(text)\n",
    "    text = araby.strip_diacritics(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    #text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    return text\n",
    "\n",
    "def strip_all(text):\n",
    "    l = [' ', '0', '1', '2', '3', '4', '5', '6',\n",
    "       '7', '8', '9', '?', \n",
    "       'ØŸ', 'Ø¡', 'Ø¤', 'Ø¦', 'Ø§', 'Ø¨', 'Øª', 'Ø«',\n",
    "       'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸',\n",
    "       'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'Ù ', 'Ù¡',\n",
    "       'Ù¢', 'Ù£', 'Ù¤', 'Ù¥', 'Ù¦', 'Ù§', 'Ù¨', 'Ù©']\n",
    "    return \"\".join([x for x in text if x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea115c1-fddf-49f7-ba66-db45bff6c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "train.tweet = train.tweet.apply(normalize).apply(strip_all).apply(araby.tokenize)\n",
    "test.tweet = test.tweet.apply(normalize).apply(strip_all).apply(araby.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b6b7fb-6f01-4b77-9334-65fd3d346dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train.label)\n",
    "train.label = le.transform(train.label)\n",
    "test.label = le.transform(test.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9c3e9a-66e2-45fa-915e-2475f3785d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ù†Ø­Ù†, Ø§Ù„Ø°ÙŠÙ†, ÙŠØªØ­ÙˆÙ„, ÙƒÙ„, Ù…Ø§, Ù†ÙˆØ¯, Ø§Ù†, Ù†Ù‚ÙˆÙ„Ù‡, Ø§Ù„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[ÙˆÙÙŠ, Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡, Ù„Ù†, ÙŠØ¨Ù‚Ø§, Ù…Ø¹Ùƒ, Ø§Ø­Ø¯Ø§Ù„Ø§, Ù…Ù†, Ø±Ø§Ø§,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ù…Ù†, Ø§Ù„Ø®ÙŠØ±, Ù†ÙØ³Ù‡]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ø²Ù„Ø²Ù„Ø§Ù„Ù…Ù„Ø¹Ø¨Ù†ØµØ±Ù†Ø§Ø¨ÙŠÙ„Ø¹Ø¨, ÙƒÙ†, Ø¹Ø§Ù„ÙŠ, Ø§Ù„Ù‡Ù…Ù‡, ÙˆÙ„Ø§, Øª...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[Ø§Ù„Ø´ÙŠØ¡, Ø§Ù„ÙˆØ­ÙŠØ¯, Ø§Ù„Ø°ÙŠ, ÙˆØµÙ„ÙˆØ§, ÙÙŠÙ‡, Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠÙ‡, Ù‡Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>0</td>\n",
       "      <td>[ÙƒÙŠÙ, ØªØ±Ø§, Ø§ÙˆØ±Ø§Ù†ÙˆØ³, Ù„Ùˆ, ÙƒØ§Ù†, ÙŠÙ‚Ø¹, Ù…ÙƒØ§Ù†, Ø§Ù„Ù‚Ù…Ø±,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ø§Ø­Ø³Ø¯Ùƒ, Ø¹Ù„Ø§, Ø§Ù„Ø§ÙŠÙ…]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ù„Ø§ÙˆÙ„, Ù…Ø±Ù‡, Ù…Ø§, Ø¨Ù†ÙƒÙˆÙ†, Ø³ÙˆØ§]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ø¨Ù‚Ù„Ù‡, Ù„ÙŠØ´, ÙŠØ§, ÙˆØ§Ø·ÙŠ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ù‚Ø¯, Ø·Ø§Ù„, ØµØ¨Ø±ÙŠ, ÙÙŠ, Ø§Ù„Ù†ÙˆØ§, Ø§Ø°, ØªØ±ÙƒØªÙ†ÙŠ, ÙƒØ¦ÙŠØ¨Ø§, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              tweet\n",
       "0          1  [Ù†Ø­Ù†, Ø§Ù„Ø°ÙŠÙ†, ÙŠØªØ­ÙˆÙ„, ÙƒÙ„, Ù…Ø§, Ù†ÙˆØ¯, Ø§Ù†, Ù†Ù‚ÙˆÙ„Ù‡, Ø§Ù„...\n",
       "1          1  [ÙˆÙÙŠ, Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡, Ù„Ù†, ÙŠØ¨Ù‚Ø§, Ù…Ø¹Ùƒ, Ø§Ø­Ø¯Ø§Ù„Ø§, Ù…Ù†, Ø±Ø§Ø§,...\n",
       "2          1                                  [Ù…Ù†, Ø§Ù„Ø®ÙŠØ±, Ù†ÙØ³Ù‡]\n",
       "3          1  [Ø²Ù„Ø²Ù„Ø§Ù„Ù…Ù„Ø¹Ø¨Ù†ØµØ±Ù†Ø§Ø¨ÙŠÙ„Ø¹Ø¨, ÙƒÙ†, Ø¹Ø§Ù„ÙŠ, Ø§Ù„Ù‡Ù…Ù‡, ÙˆÙ„Ø§, Øª...\n",
       "4          1  [Ø§Ù„Ø´ÙŠØ¡, Ø§Ù„ÙˆØ­ÙŠØ¯, Ø§Ù„Ø°ÙŠ, ÙˆØµÙ„ÙˆØ§, ÙÙŠÙ‡, Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠÙ‡, Ù‡Ùˆ...\n",
       "...      ...                                                ...\n",
       "22509      0  [ÙƒÙŠÙ, ØªØ±Ø§, Ø§ÙˆØ±Ø§Ù†ÙˆØ³, Ù„Ùˆ, ÙƒØ§Ù†, ÙŠÙ‚Ø¹, Ù…ÙƒØ§Ù†, Ø§Ù„Ù‚Ù…Ø±,...\n",
       "22510      0                                [Ø§Ø­Ø³Ø¯Ùƒ, Ø¹Ù„Ø§, Ø§Ù„Ø§ÙŠÙ…]\n",
       "22511      0                        [Ù„Ø§ÙˆÙ„, Ù…Ø±Ù‡, Ù…Ø§, Ø¨Ù†ÙƒÙˆÙ†, Ø³ÙˆØ§]\n",
       "22512      0                              [Ø¨Ù‚Ù„Ù‡, Ù„ÙŠØ´, ÙŠØ§, ÙˆØ§Ø·ÙŠ]\n",
       "22513      0  [Ù‚Ø¯, Ø·Ø§Ù„, ØµØ¨Ø±ÙŠ, ÙÙŠ, Ø§Ù„Ù†ÙˆØ§, Ø§Ø°, ØªØ±ÙƒØªÙ†ÙŠ, ÙƒØ¦ÙŠØ¨Ø§, ...\n",
       "\n",
       "[45275 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3ff382-6aca-414a-936b-2f93adcfd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train.tweet.values, train.label.values, test_size=0.5,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01b5856-3e86-4d12-bb77-779ffbce575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding import WordEmbedding\n",
    "from utils import helper, preprocess\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3d4b35-a57a-4840-8738-5d85bcf50e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42394/42394 [00:00<00:00, 44189.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5473)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                54740     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5473)              60203     \n",
      "=================================================================\n",
      "Total params: 114,943\n",
      "Trainable params: 114,943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1325/1325 [==============================] - 6s 4ms/step - loss: 0.2986 - accuracy: 0.0081\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.11875, saving model to models/word_embeddings.h5\n",
      "Epoch 2/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0038 - accuracy: 0.0214\n",
      "\n",
      "Epoch 00002: loss improved from 0.11875 to 0.00305, saving model to models/word_embeddings.h5\n",
      "Epoch 3/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0020 - accuracy: 0.0224\n",
      "\n",
      "Epoch 00003: loss improved from 0.00305 to 0.00187, saving model to models/word_embeddings.h5\n",
      "Epoch 4/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0017 - accuracy: 0.0214\n",
      "\n",
      "Epoch 00004: loss improved from 0.00187 to 0.00168, saving model to models/word_embeddings.h5\n",
      "Epoch 5/5\n",
      "1325/1325 [==============================] - 5s 4ms/step - loss: 0.0016 - accuracy: 0.0209\n",
      "\n",
      "Epoch 00005: loss improved from 0.00168 to 0.00163, saving model to models/word_embeddings.h5\n"
     ]
    }
   ],
   "source": [
    "# Word2vec\n",
    "embeddings = WordEmbedding(preprocess.tokenizer, vocab_size=13000, maxlen=150, embedding_vector=10, method=\"word2vec\")\n",
    "#text = embeddings.tokenize(text) We already did tokenization\n",
    "words, label, unique_words, word_dict = embeddings.encode_w2v(train.tweet.values[:1000]) #Consumes very large amount of memory\n",
    "model = embeddings.train_w2v(words, label, epochs=5)\n",
    "\n",
    "word_embeddings = model.get_weights()[0]\n",
    "\n",
    "# embeddings = helper.get_embeddings(unique_words, word_dict, word_embeddings)\n",
    "# helper.plot(word_dict, embeddings)\n",
    "# helper.save_embeddings(embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b6ba0efc-2a73-480a-ba4f-92839c6c0ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606585, 652324)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "sentences = np.concatenate([train.tweet.values, test.tweet.values])\n",
    "word_model = gensim.models.Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "word_model.train(sentences, total_examples=word_model.corpus_count, epochs=1)  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cfa19dd1-9aa3-4981-9625-45cdf304610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word_model.syn1neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3c926f74-64d0-4d68-9774-f28fa11bcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.key_to_index[word]\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index_to_key[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3ed6f2e-e8d2-4b25-ac8d-3a786b7b4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tmp = np.zeros([X_train.shape[0], 150], dtype=np.int32)\n",
    "cnt,cntt=0,0\n",
    "for i, sentence in enumerate(X_train):\n",
    "    for t, word in enumerate(sentence[:150]):\n",
    "        if word in word_model.wv.key_to_index:\n",
    "            X_train_tmp[i, t] = word2idx(word)\n",
    "            cntt += 1\n",
    "        else:\n",
    "            X_train_tmp[i, t] = 0\n",
    "            cnt += 1\n",
    "X_train = X_train_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32925da2-ce69-436e-a1e9-bae694ee02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_tmp = np.zeros([X_valid.shape[0], 150], dtype=np.int32)\n",
    "cnt,cntt=0,0\n",
    "for i, sentence in enumerate(X_valid):\n",
    "    for t, word in enumerate(sentence[:150]):\n",
    "        if word in word_model.wv.key_to_index:\n",
    "            X_valid_tmp[i, t] = word2idx(word)\n",
    "            cntt += 1\n",
    "        else:\n",
    "            X_valid_tmp[i, t] = 0\n",
    "            cnt += 1\n",
    "X_valid = X_valid_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f200cc9d-35bf-43fb-85e2-5e93bb5c9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, emdedding_size = weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "03fa6ad9-bab8-4d50-9323-5f08bda343e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from random import shuffle\n",
    "from pyarabic import araby\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional, BatchNormalization, Flatten, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1991ac9c-02ff-42db-b9ed-f294c134ee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  145,     6,  5148, ...,     0,     0,     0],\n",
       "       [   10,   274,     4, ...,     0,     0,     0],\n",
       "       [57783,     1, 57811, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 4286, 36334,   136, ...,     0,     0,     0],\n",
       "       [   18, 33922,  4851, ...,     0,     0,     0],\n",
       "       [   89,    70,    19, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "be3d713b-e4b7-494f-8e1a-fd71662dfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((150,)))\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[weights]))\n",
    "model.add(Bidirectional(GRU(units = 32, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(units = 32, return_sequences=False)))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b7ebf0a-b447-4397-9f0a-3575e7f53ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "177/177 [==============================] - 19s 81ms/step - loss: 0.6368 - accuracy: 0.6118 - val_loss: 0.4767 - val_accuracy: 0.7558\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.75576, saving model to full_verse_7.h5\n",
      "Epoch 2/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.3280 - accuracy: 0.8634 - val_loss: 0.5353 - val_accuracy: 0.7563\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.75576 to 0.75634, saving model to full_verse_7.h5\n",
      "Epoch 3/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.1375 - accuracy: 0.9514 - val_loss: 0.6196 - val_accuracy: 0.7501\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.75634\n",
      "Epoch 4/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0812 - accuracy: 0.9720 - val_loss: 0.7676 - val_accuracy: 0.7597\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.75634 to 0.75974, saving model to full_verse_7.h5\n",
      "Epoch 5/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.8371 - val_accuracy: 0.7566\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.75974\n",
      "Epoch 6/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0526 - accuracy: 0.9796 - val_loss: 0.8895 - val_accuracy: 0.7570\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.75974\n",
      "Epoch 7/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0511 - accuracy: 0.9804 - val_loss: 0.9423 - val_accuracy: 0.7551\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.75974\n",
      "Epoch 8/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0450 - accuracy: 0.9821 - val_loss: 0.9752 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.75974\n",
      "Epoch 9/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0435 - accuracy: 0.9821 - val_loss: 1.0026 - val_accuracy: 0.7597\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.75974\n",
      "Epoch 10/15\n",
      "177/177 [==============================] - 14s 76ms/step - loss: 0.0384 - accuracy: 0.9846 - val_loss: 1.0448 - val_accuracy: 0.7572\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.75974\n",
      "Epoch 11/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0380 - accuracy: 0.9845 - val_loss: 1.0802 - val_accuracy: 0.7577\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.75974\n",
      "Epoch 12/15\n",
      "177/177 [==============================] - 13s 76ms/step - loss: 0.0372 - accuracy: 0.9830 - val_loss: 1.1148 - val_accuracy: 0.7583\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.75974\n",
      "Epoch 13/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0363 - accuracy: 0.9834 - val_loss: 1.1448 - val_accuracy: 0.7597\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.75974\n",
      "Epoch 14/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0340 - accuracy: 0.9846 - val_loss: 1.1646 - val_accuracy: 0.7553\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.75974\n",
      "Epoch 15/15\n",
      "177/177 [==============================] - 13s 75ms/step - loss: 0.0345 - accuracy: 0.9840 - val_loss: 1.2084 - val_accuracy: 0.7579\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.75974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae44406520>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.0001, min_lr=0.0001)]\n",
    "callbacks += [tf.keras.callbacks.ModelCheckpoint('gensim_w2v_scratch.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')]\n",
    "model.fit(X_train, y_train, validation_data= (X_valid, y_valid), epochs = 15, batch_size= 128, shuffle = True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c80b8-9385-4cc7-ac2d-7d248cad384a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
